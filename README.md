# ğŸ“š **Complete System Architecture & Technical Deep Dive**



---

## **1ï¸âƒ£ Complete Workflow: How a Prompt is Treated**

### **End-to-End Flow Diagram:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USER INPUT: "Show me my orders"                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FRONTEND (index.html)                                           â”‚
â”‚ â€¢ Captures user message                                         â”‚
â”‚ â€¢ Retrieves session_id from localStorage                        â”‚
â”‚ â€¢ Sends POST /chat request                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP POST {user_message, session_id}
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BACKEND: app.py - FastAPI Endpoint                              â”‚
â”‚                                                                  â”‚
â”‚ Step 1: Session Validation                                      â”‚
â”‚   â€¢ Check if session_id exists                                  â”‚
â”‚   â€¢ Create anonymous session if none                            â”‚
â”‚   â€¢ Get user_id from session (if logged in)                     â”‚
â”‚                                                                  â”‚
â”‚ Step 2: Guardrails (guardrails.py)                             â”‚
â”‚   â€¢ Safety check (violence, self-harm)                          â”‚
â”‚   â€¢ Inappropriate content (dating, sexual)                      â”‚
â”‚   â€¢ Domain check (ecommerce-related?)                           â”‚
â”‚   â€¢ âœ… PASS â†’ Continue                                          â”‚
â”‚   â€¢ âŒ BLOCK â†’ Return rejection message                         â”‚
â”‚                                                                  â”‚
â”‚ Step 3: Load Conversation History                              â”‚
â”‚   â€¢ db.get_conversation_history(session_id, limit=20)          â”‚
â”‚   â€¢ Returns last 20 messages from SQLite                        â”‚
â”‚                                                                  â”‚
â”‚ Step 4: Save User Message                                      â”‚
â”‚   â€¢ db.add_message(session_id, role="user", content=...)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ORCHESTRATOR: agent/orchestrator.py                             â”‚
â”‚                                                                  â”‚
â”‚ Step 5: Quick Intent Detection (router.py)                     â”‚
â”‚   â€¢ Pattern matching: "my orders" â†’ order_status intent        â”‚
â”‚   â€¢ Extract entities: order_id, user_id, category              â”‚
â”‚   â€¢ Purpose: Logging & fallback routing                         â”‚
â”‚                                                                  â”‚
â”‚ Step 6: Handle Simple Cases (No Agentic Loop)                  â”‚
â”‚   â€¢ Chitchat: "hi", "hello" â†’ Return greeting                  â”‚
â”‚   â€¢ Date query: "what's today?" â†’ Return date                  â”‚
â”‚                                                                  â”‚
â”‚ Step 7: Check Authentication Requirement                        â”‚
â”‚   â€¢ Is this a PRIVATE_INTENT? (orders, returns, refunds)       â”‚
â”‚   â€¢ Is user_id available? â†’ YES â†’ Continue                     â”‚
â”‚                            â†’ NO â†’ Ask for login/user_id        â”‚
â”‚                                                                  â”‚
â”‚ Step 8: AGENTIC LOOP (Main Intelligence)                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚ A. Configure Gemini with Function Calling           â”‚     â”‚
â”‚   â”‚    â€¢ Load GEMINI_API_KEY                            â”‚     â”‚
â”‚   â”‚    â€¢ Create GenerativeModel with tool definitions   â”‚     â”‚
â”‚   â”‚                                                      â”‚     â”‚
â”‚   â”‚ B. Build System Prompt                              â”‚     â”‚
â”‚   â”‚    â€¢ Include user_id context (if logged in)         â”‚     â”‚
â”‚   â”‚    â€¢ List available tools                           â”‚     â”‚
â”‚   â”‚    â€¢ Add behavioral guidelines                      â”‚     â”‚
â”‚   â”‚                                                      â”‚     â”‚
â”‚   â”‚ C. Format Conversation History                      â”‚     â”‚
â”‚   â”‚    â€¢ Convert DB format â†’ Gemini format              â”‚     â”‚
â”‚   â”‚    â€¢ user messages â†’ {"role": "user", ...}          â”‚     â”‚
â”‚   â”‚    â€¢ assistant messages â†’ {"role": "model", ...}    â”‚     â”‚
â”‚   â”‚                                                      â”‚     â”‚
â”‚   â”‚ D. Start Agentic Loop (Max 10 iterations)           â”‚     â”‚
â”‚   â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚     â”‚
â”‚   â”‚    â”‚ Iteration 1:                             â”‚    â”‚     â”‚
â”‚   â”‚    â”‚ â€¢ Send: system_prompt + user_message     â”‚    â”‚     â”‚
â”‚   â”‚    â”‚ â€¢ Gemini decides: "I need to find orders"â”‚   â”‚     â”‚
â”‚   â”‚    â”‚ â€¢ Returns: function_call(                â”‚    â”‚     â”‚
â”‚   â”‚    â”‚     name="find_orders_by_user_id",       â”‚    â”‚     â”‚
â”‚   â”‚    â”‚     args={"user_id": "U001"}             â”‚    â”‚     â”‚
â”‚   â”‚    â”‚   )                                       â”‚    â”‚     â”‚
â”‚   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚     â”‚
â”‚   â”‚               â”‚                                     â”‚     â”‚
â”‚   â”‚               â–¼                                     â”‚     â”‚
â”‚   â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚     â”‚
â”‚   â”‚    â”‚ Execute Tool (execute_tool_call)         â”‚    â”‚     â”‚
â”‚   â”‚    â”‚ â€¢ Maps tool name â†’ actual function       â”‚    â”‚     â”‚
â”‚   â”‚    â”‚ â€¢ Calls find_orders_by_user_id_tool      â”‚    â”‚     â”‚
â”‚   â”‚    â”‚ â€¢ Returns: {found: true, orders: [...]}  â”‚    â”‚     â”‚
â”‚   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚     â”‚
â”‚   â”‚               â”‚                                     â”‚     â”‚
â”‚   â”‚               â–¼                                     â”‚     â”‚
â”‚   â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚     â”‚
â”‚   â”‚    â”‚ Iteration 2:                             â”‚    â”‚     â”‚
â”‚   â”‚    â”‚ â€¢ Send function_response to Gemini       â”‚    â”‚     â”‚
â”‚   â”‚    â”‚ â€¢ Gemini processes tool result           â”‚    â”‚     â”‚
â”‚   â”‚    â”‚ â€¢ Decides: "I have the answer now"       â”‚    â”‚     â”‚
â”‚   â”‚    â”‚ â€¢ Returns: text response                 â”‚    â”‚     â”‚
â”‚   â”‚    â”‚   "You have 2 orders: ORD1001..."        â”‚    â”‚     â”‚
â”‚   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚     â”‚
â”‚   â”‚                                                      â”‚     â”‚
â”‚   â”‚ E. Loop Exits When:                                 â”‚     â”‚
â”‚   â”‚    â€¢ Gemini returns text (final answer)             â”‚     â”‚
â”‚   â”‚    â€¢ Max 10 iterations reached                      â”‚     â”‚
â”‚   â”‚    â€¢ Error occurs                                   â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                â”‚
â”‚ Step 9: Fallback Handling                                     â”‚
â”‚   â€¢ If agentic loop fails â†’ RAG fallback                      â”‚
â”‚   â€¢ answer_with_rag(user_message, k=3)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SAVE RESPONSE                                                   â”‚
â”‚ â€¢ db.add_message(session_id, role="assistant", content=...)    â”‚
â”‚ â€¢ Store in conversation_history table                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RETURN TO FRONTEND                                              â”‚
â”‚ {                                                               â”‚
â”‚   "answer": "You have 2 orders...",                            â”‚
â”‚   "intent": "order_status",                                    â”‚
â”‚   "route": "gemini:agentic_function_calling",                  â”‚
â”‚   "tool_calls": [{tool: "find_orders_by_user_id", ...}],      â”‚
â”‚   "iterations": 2                                              â”‚
â”‚ }                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## **2ï¸âƒ£ Why Gemini Agentic Flow Over LangGraph?**

### **Comparison Table:**

| Criteria | Gemini Function Calling | LangGraph | LangChain ReAct |
|----------|------------------------|-----------|-----------------|
| **Setup Complexity** | â­ Simple | â­â­â­ Complex | â­â­ Medium |
| **Learning Curve** | Easy | Steep | Medium |
| **Control Over Flow** | Medium | High | Medium |
| **Performance** | Fast | Medium | Medium |
| **Debugging** | Easy | Hard | Medium |
| **Token Efficiency** | High | Medium | Medium |
| **Multi-step Support** | âœ… Yes | âœ… Yes | âœ… Yes |
| **State Management** | Built-in | Explicit | Built-in |

### **Why We Chose Gemini:**

**1. Native Integration** âœ…
- Already using Gemini 2.0 for main LLM
- Function calling is native feature
- No additional dependencies

**2. Simplicity** âœ…
- Less boilerplate code (~100 lines vs 300+ for LangGraph)
- Easier to explain in viva
- Faster development

**3. Performance** âœ…
- Direct API calls (no wrapper overhead)
- Optimized for Gemini's architecture
- Lower latency

**4. Token Efficiency** âœ…
- Gemini manages context internally
- No need to manually reconstruct state graphs
- Cheaper per request

**5. Reliability** âœ…
- Fewer moving parts = fewer bugs
- Google-maintained (stable API)
- Better error handling

### **When Would You Use LangGraph Instead?**

- **Complex workflows** with 20+ steps
- **Human-in-the-loop** approval needed
- **Parallel execution** of tools required
- **Visual debugging** of workflow needed
- **Production systems** with compliance requirements

### **Agentic Behavior Proof:**

**Traditional (Rule-based):**
```python
if "order" in query:
    call get_order_tool()
```

**Our Agentic Approach:**
```python
# LLM DECIDES autonomously:
"User wants orders â†’ I'll call find_orders_by_user_id
 â†’ Got 2 orders â†’ Now I'll format them nicely
 â†’ Done, here's the answer"
```

**Evidence in Logs:**
```
[Agent] Calling tool: find_orders_by_user_id  â† LLM decided this
[Agent] Tool result: {...}
[Agent] Calling tool: check_return_eligibility  â† LLM chained another tool
```

---

## **3ï¸âƒ£ User Data Storage & Retrieval**

### **Database Schema (SQLite):**

```sql
-- 1. Users Table
users (
    user_id TEXT PRIMARY KEY,     -- U001, U002
    name TEXT,
    email TEXT UNIQUE,
    password_hash TEXT            -- bcrypt hashed
)

-- 2. Sessions Table
sessions (
    session_id TEXT PRIMARY KEY,  -- UUID
    user_id TEXT,                 -- NULL for anonymous
    created_at TIMESTAMP,
    last_active TIMESTAMP,
    is_active BOOLEAN
)

-- 3. Conversation History
conversation_history (
    id INTEGER PRIMARY KEY,
    session_id TEXT,
    user_id TEXT,                 -- Denormalized for queries
    role TEXT,                    -- 'user' or 'assistant'
    content TEXT,
    intent TEXT,
    route TEXT,
    timestamp TIMESTAMP
)

-- 4. Conversation State (for multi-step flows)
conversation_state (
    session_id TEXT PRIMARY KEY,
    current_intent TEXT,
    awaiting_field TEXT,
    collected_slots TEXT          -- JSON
)
```

### **Data Flow:**

**Login Flow:**
```python
# 1. User submits login
POST /login {email: "abhinav@example.com", password: "demo123"}

# 2. Backend validates
user = db.get_user_by_email(email)
verify_password(password, user.password_hash)  # bcrypt

# 3. Create session
session_id = db.create_session(user_id="U001")

# 4. Return to frontend
return {session_id: "abc-123", user_id: "U001", name: "Abhinav"}

# 5. Frontend stores
localStorage.setItem("session_id", "abc-123")
localStorage.setItem("current_user", JSON.stringify({...}))
```

**Session Validation on Each Request:**
```python
# In app.py - chat endpoint
session_id = payload.session_id
user_id = get_session_user(session_id)  # Query DB

# database/db_manager.py
def get_session_user(session_id):
    session = db.execute(
        "SELECT user_id FROM sessions WHERE session_id=? AND is_active=TRUE",
        (session_id,)
    )
    return session.user_id if session else None
```

### **Order Data Retrieval:**

```python
# Structured data: data/structured/orders.json
[
  {
    "order_id": "ORD1001",
    "user_id": "U001",  â† Linked to user
    "items": [...],
    "order_date": "2025-11-20",
    "status": "delivered"
  }
]

# Tool: tools/user_tool.py
def find_orders_by_user_id(user_id):
    orders = load_orders()  # Read JSON
    return [o for o in orders if o["user_id"] == user_id]
```

---

## **4ï¸âƒ£ Context Preservation**

### **Three Levels of Context:**

#### **Level 1: Session Context (Database)**
```python
# Every message stored in DB
db.add_message(
    session_id="abc-123",
    role="user",
    content="Show my orders"
)
db.add_message(
    session_id="abc-123",
    role="assistant",
    content="You have 2 orders..."
)

# Retrieved on next request
history = db.get_conversation_history(session_id, limit=20)
# Returns last 20 messages in chronological order
```

#### **Level 2: In-Memory Context (Current Request)**
```python
# Format for Gemini
formatted_history = [
    {"role": "user", "parts": [{"text": "Show my orders"}]},
    {"role": "model", "parts": [{"text": "You have 2 orders..."}]},
    {"role": "user", "parts": [{"text": "Return the laptop"}]}
]

# Sent to Gemini
chat = model.start_chat(history=formatted_history)
response = chat.send_message(new_message)
```

#### **Level 3: User Identity Context**
```python
# Injected into system prompt
system_prompt = f"""
User is logged in as: {user_id}
When user says "my orders", use user_id automatically.
"""
```

### **Context Preservation Example:**

**Turn 1:**
```
User: "Show my orders"
History: []
Agent: [Calls find_orders_by_user_id(U001)]
Response: "You have 2 orders: ORD1001, ORD1002"
DB: Saves both messages
```

**Turn 2:**
```
User: "Return the laptop"
History: [Turn 1 messages]
Agent: "Laptop is in ORD1001, checking return eligibility..."
       [Calls check_return_eligibility(ORD1001)]
Response: "ORD1001 is outside return window"
DB: Saves both messages
```

**Turn 3:**
```
User: "What about refund?"
History: [Turn 1, Turn 2 messages]
Agent: "User wants refund for ORD1001 (from context)"
       [Calls check_refund_possibility(ORD1001)]
```

---

## **5ï¸âƒ£ Session Memory**

### **Storage Location:**
```
data/assistant.db (SQLite file)
```

### **How It Works:**

**1. Session Creation:**
```python
# When user opens chat (no login)
session_id = str(uuid.uuid4())  # "abc-123-def-456"
db.execute("INSERT INTO sessions (session_id, user_id) VALUES (?, NULL)")

# When user logs in
db.execute("UPDATE sessions SET user_id = ? WHERE session_id = ?", (user_id, session_id))
```

**2. Message Storage:**
```python
# After every chat turn
db.execute("""
    INSERT INTO conversation_history (session_id, user_id, role, content)
    VALUES (?, ?, ?, ?)
""", (session_id, user_id, role, content))
```

**3. Message Retrieval:**
```python
# Before processing new message
cursor = db.execute("""
    SELECT role, content FROM conversation_history
    WHERE session_id = ?
    ORDER BY timestamp DESC
    LIMIT 20
""", (session_id,))

history = list(reversed(cursor.fetchall()))  # Chronological order
```

### **Session Lifecycle:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Opens Chat â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Create Anonymous Sessionâ”‚  session_id = uuid4()
â”‚ Store in localStorage   â”‚  user_id = NULL
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Logs In    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Link Session to User    â”‚  UPDATE sessions SET user_id = "U001"
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Chats (5 messages) â”‚  All saved to conversation_history
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Closes Tab â”‚  Session remains in DB
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Returns Next Day   â”‚  session_id still in localStorage
â”‚ Context Restored!       â”‚  Load last 20 messages from DB
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Memory Limits:**

- **Per Session:** Last 20 messages (configurable)
- **Token Limit:** ~30k tokens (Gemini 2.0 context window)
- **Database:** Unlimited (until disk space)

### **Why Last 20 Messages?**

- **Balance:** Enough context without overwhelming LLM
- **Performance:** Fast retrieval from DB
- **Token Efficiency:** ~5k-10k tokens average
- **Relevance:** Recent messages most important

---

## **6ï¸âƒ£ How RAG Comes Into Picture**

### **RAG Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAG KNOWLEDGE BASE                                        â”‚
â”‚                                                            â”‚
â”‚ 1. Raw Documents (data/raw/)                              â”‚
â”‚    â€¢ return_policy.txt                                    â”‚
â”‚    â€¢ shipping_policy.txt                                  â”‚
â”‚    â€¢ warranty_terms.txt                                   â”‚
â”‚    â€¢ general_faq.txt                                      â”‚
â”‚                                                            â”‚
â”‚ 2. Chunking (rag/chunking.py)                            â”‚
â”‚    â€¢ Chunk size: 1000 characters                          â”‚
â”‚    â€¢ Overlap: 200 characters                              â”‚
â”‚    â€¢ Result: 15-20 chunks                                 â”‚
â”‚                                                            â”‚
â”‚ 3. Embedding (rag/embeddings.py)                         â”‚
â”‚    â€¢ Model: all-MiniLM-L6-v2                             â”‚
â”‚    â€¢ Dimension: 384                                       â”‚
â”‚    â€¢ Speed: ~5ms per chunk                                â”‚
â”‚                                                            â”‚
â”‚ 4. Vector Store (rag/vectorstore.py)                     â”‚
â”‚    â€¢ ChromaDB (persistent)                                â”‚
â”‚    â€¢ Location: data/vectorstore/                          â”‚
â”‚    â€¢ Similarity: Cosine                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **When RAG is Used:**

**Scenario 1: Policy Questions**
```python
User: "What is your return policy?"

# Orchestrator detects: policy_question intent
# OR: Agentic loop calls search_policy_docs tool

# RAG Pipeline:
1. Embed query â†’ [0.23, -0.45, 0.78, ...]  (384 dims)
2. Search ChromaDB â†’ Top 3 similar chunks
3. Retrieved chunks:
   - Chunk 1: "Return policy allows 7 days..." (similarity: 0.92)
   - Chunk 2: "Electronics can be returned..." (similarity: 0.85)
   - Chunk 3: "Refund processed within 5-7..." (similarity: 0.78)
4. Build prompt:
   Context: [Chunk 1 + Chunk 2 + Chunk 3]
   Question: "What is your return policy?"
5. Send to Gemini â†’ Generate answer
```

**Scenario 2: Troubleshooting (3-Tier)**
```python
User: "My laptop screen is flickering"

Tier 1: Structured tool (troubleshooting.json)
  â†’ Not found (no entry for "flickering")

Tier 2: RAG (search manuals)
  â†’ Search vectorstore for "laptop screen flickering"
  â†’ Found relevant chunks â†’ Return answer

Tier 3: LLM Generic (if RAG fails)
  â†’ Generate general troubleshooting steps
```

### **RAG Flow Diagram:**

```
User Query: "What is warranty period?"
         â†“
    Embed Query
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vector Search     â”‚
â”‚  ChromaDB          â”‚
â”‚  Cosine Similarity â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
Retrieved Chunks (Top 3):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Doc 1] "Laptops have 1 year..."     â”‚
â”‚ [Doc 2] "Headphones have 6 months..." â”‚
â”‚ [Doc 3] "Warranty starts from..."    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
Build RAG Prompt:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ You are an assistant.                â”‚
â”‚ Use ONLY the following context:      â”‚
â”‚                                      â”‚
â”‚ [Doc 1] Laptops have 1 year...      â”‚
â”‚ [Doc 2] Headphones have 6 months... â”‚
â”‚ [Doc 3] Warranty starts from...     â”‚
â”‚                                      â”‚
â”‚ User question:                       â”‚
â”‚ "What is warranty period?"           â”‚
â”‚                                      â”‚
â”‚ Answer based ONLY on context above.  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    Send to Gemini
         â†“
    Generate Answer
         â†“
"Laptops have a 1-year warranty, while 
 headphones have a 6-month warranty from 
 the date of delivery. [Doc 1][Doc 2]"
```

---

## **7ï¸âƒ£ RAG + Agentic + LLM: Individual & Combined Working**

### **Individual Components:**

#### **A. LLM (Gemini 2.0) - Brain**
**Role:** Language understanding & generation

**Capabilities:**
- Understand natural language
- Generate coherent responses
- Follow instructions
- Reason about context

**Limitations:**
- âŒ No real-time data
- âŒ Can't access databases
- âŒ Can't execute actions
- âŒ Knowledge cutoff (Jan 2025)

#### **B. RAG (Retrieval-Augmented Generation) - Memory**
**Role:** Provide grounded, factual information

**Capabilities:**
- âœ… Access company policies
- âœ… Retrieve product manuals
- âœ… Find FAQs
- âœ… Ground answers in documents

**Limitations:**
- âŒ Can't handle transactional queries (order status)
- âŒ No user-specific data
- âŒ Static knowledge only

#### **C. Agentic Workflow - Hands**
**Role:** Take actions, use tools

**Capabilities:**
- âœ… Query databases
- âœ… Check order status
- âœ… Calculate return eligibility
- âœ… Chain multiple actions

**Limitations:**
- âŒ Needs LLM to decide what to do
- âŒ Can't generate natural language

### **Combined Power:**

**Example 1: Simple Product Recommendation**
```
User: "Recommend a laptop under 60000"

LLM: "I should use the search_products tool"
     â†“
Agentic: Calls search_products_tool(category="laptop", max_price=60000)
     â†“
Tool Returns: [ZenBook 14, ThinkPro 15]
     â†“
LLM: "Based on the catalog, I recommend ZenBook 14 (â‚¹69,990) 
      and ThinkPro 15 (â‚¹54,990) for your budget..."
```

**Example 2: Policy Question with Context**
```
User: "Can I return my laptop?"

LLM: "I need the user's order first"
     â†“
Agentic: Calls find_orders_by_user_id(U001)
     â†“
Tool Returns: [{order_id: "ORD1001", delivery_date: "2025-11-25"}]
     â†“
Agentic: Calls check_return_eligibility(ORD1001, today="2025-12-07")
     â†“
Tool Returns: {eligible: false, reason: "Outside 7-day window"}
     â†“
LLM: "I should check the policy for exceptions"
     â†“
RAG: Search "return policy exceptions"
     â†“
RAG Returns: "No exceptions for electronics after 7 days"
     â†“
LLM: "Your laptop order ORD1001 was delivered on Nov 25, which 
      is 12 days ago. Unfortunately, our return policy only 
      allows returns within 7 days of delivery, with no 
      exceptions for electronics."
```

**Example 3: Complex Troubleshooting**
```
User: "My laptop is overheating and making noise"

LLM: "This is a troubleshooting query"
     â†“
Agentic: Calls get_troubleshooting_steps("laptop", "overheating")
     â†“
Tool Returns: [
  "Ensure vents are not blocked",
  "Clean dust from vents",
  "Close heavy applications"
]
     â†“
LLM: "The structured steps don't mention noise, let me search docs"
     â†“
RAG: Search "laptop noise overheating"
     â†“
RAG Returns: Chunks about fan maintenance
     â†“
LLM: Synthesizes both sources:
     "For overheating:
      1. Ensure vents are not blocked
      2. Clean dust from vents
      3. Close heavy applications
      
      For the noise:
      The fan might be clogged with dust or failing. Try 
      cleaning the vents first. If noise persists, the fan 
      may need replacement - please contact support."
```

---

## **8ï¸âƒ£ Guardrails: How They Work**

### **Architecture:**

```
User Input
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GUARDRAILS (guardrails.py)        â”‚
â”‚                                    â”‚
â”‚ 1. Empty Check                     â”‚
â”‚    if not text: BLOCK              â”‚
â”‚                                    â”‚
â”‚ 2. Safety Filter                   â”‚
â”‚    keywords: ["suicide", "bomb"]   â”‚
â”‚    if match: BLOCK                 â”‚
â”‚                                    â”‚
â”‚ 3. Inappropriate Content           â”‚
â”‚    keywords: ["date a girl",       â”‚
â”‚               "pickup line"]       â”‚
â”‚    if match: BLOCK                 â”‚
â”‚                                    â”‚
â”‚ 4. Domain Check                    â”‚
â”‚    if not (ecommerce OR chitchat): â”‚
â”‚       BLOCK                         â”‚
â”‚                                    â”‚
â”‚ âœ… PASS â†’ Continue to orchestrator â”‚
â”‚ âŒ BLOCK â†’ Return rejection msg    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Implementation:**

```python
# guardrails.py

BLOCKED_KEYWORDS = [
    "suicide", "kill myself", "bomb", "terrorist"
]

DATING_KEYWORDS = [
    "date a girl", "pickup line", "flirt with"
]

ECOMMERCE_KEYWORDS = [
    "order", "return", "refund", "product", "laptop"
]

CHITCHAT_KEYWORDS = [
    "hi", "hello", "how are you"
]

def apply_guardrails(user_message: str) -> GuardrailResult:
    lowered = user_message.lower()
    
    # Safety
    if any(kw in lowered for kw in BLOCKED_KEYWORDS):
        return GuardrailResult(
            allowed=False,
            reason="safety",
            message="I can't help with that. Please contact emergency services."
        )
    
    # Inappropriate
    if any(kw in lowered for kw in DATING_KEYWORDS):
        return GuardrailResult(
            allowed=False,
            reason="inappropriate",
            message="I'm designed for ecommerce support only."
        )
    
    # Domain
    has_ecommerce = any(kw in lowered for kw in ECOMMERCE_KEYWORDS)
    has_chitchat = any(kw in lowered for kw in CHITCHAT_KEYWORDS)
    
    if not (has_ecommerce or has_chitchat):
        return GuardrailResult(
            allowed=False,
            reason="out_of_domain",
            message="Please ask questions about orders, products, or support."
        )
    
    return GuardrailResult(allowed=True)
```

### **Guardrail Statistics:**

| Category | Example | Action |
|----------|---------|--------|
| Safety | "how to make a bomb" | âŒ BLOCK |
| Inappropriate | "give me pickup lines" | âŒ BLOCK |
| Out of Domain | "tell me a joke" | âŒ BLOCK (unless chitchat detected) |
| Empty | "" | âŒ BLOCK |
| Valid Ecommerce | "show my orders" | âœ… PASS |
| Valid Chitchat | "hello" | âœ… PASS |

---

## **9ï¸âƒ£ Retrieval Strategy**

### **Retrieval Method: Semantic Similarity (Cosine)**

**Formula:**
```
similarity = (vector_A Â· vector_B) / (||vector_A|| Ã— ||vector_B||)

Range: [-1, 1]
- 1.0 = Identical
- 0.0 = Orthogonal (unrelated)
- -1.0 = Opposite
```

### **Retrieval Parameters:**

```python
# In rag_chain.py
def answer_with_rag(question: str, k: int = 3):
    retriever = vectordb.as_retriever(
        search_kwargs={
            "k": 3  # Top 3 most similar chunks
        }
    )
```

**Why k=3?**
- âœ… Enough context (3 chunks Ã— 1000 chars = 3000 chars)
- âœ… Manageable token count (~750 tokens)
- âœ… Diverse perspectives
- âœ… Reduces noise

### **Retrieval Pipeline:**

```
1. Query: "What is return policy?"
   â†“
2. Embed: [0.23, -0.45, 0.78, ..., 0.12]  (384 dimensions)
   â†“
3. Search ChromaDB:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Chunk 1: "Return policy states..." â”‚ 0.92
   â”‚ Chunk 2: "Electronics can be..."   â”‚ 0.85
   â”‚ Chunk 3: "Refunds processed in..."  â”‚ 0.78
   â”‚ Chunk 4: "Shipping takes 3-5..."    â”‚ 0.45 â† Not retrieved
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
4. Return Top 3
```

### **Retrieval Evaluation Metrics:**

**Recall@k:**
```
Recall@3 = (Relevant docs retrieved) / (Total relevant docs)

Example:
- Relevant docs in DB: 4
- Retrieved: 3
- Recall@3 = 3/4 = 0.75 (75%)
```

**Precision@k:**
```
Precision@3 = (Relevant docs retrieved) / (Docs retrieved)

Example:
- Retrieved: 3
- Relevant: 3
- Precision@3 = 3/3 = 1.0 (100%)
```

---

## **ğŸ”Ÿ Tokenization Strategy**

### **Two Levels of Tokenization:**

#### **Level 1: Document Chunking (Character-based)**
```python
# In rag/chunking.py
RecursiveCharacterTextSplitter(
    chunk_size=1000,      # characters
    chunk_overlap=200,    # characters
    length_function=len   # character count
)
```

**Why Character-based?**
- âœ… Simple and predictable
- âœ… Language-agnostic
- âœ… Fast
- âš ï¸ Approximates 250-300 words per chunk

#### **Level 2: LLM Tokenization (Gemini BPE)**

**Gemini uses Byte-Pair Encoding (BPE):**
```
Text: "Show me my orders"

Tokenization:
["Show", " me", " my", " orders"]
â†’ [12345, 67890, 11223, 44556]

Token count: 4
```

### **Token Limits:**

| Component | Limit | Typical Usage |
|-----------|-------|---------------|
| **Gemini Input** | 32,768 tokens | 5,000-10,000 |
| **Gemini Output** | 8,192 tokens | 500-1,500 |
| **Single Chunk** | ~250 tokens | (1000 chars) |
| **History (20 msgs)** | ~5,000 tokens | Variable |
| **System Prompt** | ~500 tokens | Fixed |
| **Tool Definitions** | ~1,000 tokens | Fixed |
| **Total per Request** | ~8,000 tokens | Safe margin |

### **Token Budget Breakdown:**

```
Typical Request:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ System Prompt              â”‚   500  â”‚
â”‚ Tool Definitions (8 tools) â”‚ 1,000  â”‚
â”‚ Conversation History (20)  â”‚ 5,000  â”‚
â”‚ User Message               â”‚   100  â”‚
â”‚ RAG Context (3 chunks)     â”‚   750  â”‚
â”‚ Tool Results               â”‚   500  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL INPUT                â”‚ 7,850  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Response:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Assistant Message          â”‚   500  â”‚
â”‚ Function Calls (metadata)  â”‚   200  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL OUTPUT               â”‚   700  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

GRAND TOTAL: 8,550 tokens (well under 32k limit)
```

---

## **1ï¸âƒ£1ï¸âƒ£ LLM Calls per Cycle**

### **Scenario Analysis:**

#### **Scenario 1: Simple Chitchat**
```
User: "Hello"
â””â”€ 1 LLM call (direct response)

Total: 1 call
```

#### **Scenario 2: Product Search**
```
User: "Show laptops under 60000"
â”œâ”€ Call 1: Gemini decides to use search_products tool
â”œâ”€ [Tool execution - not an LLM call]
â””â”€ Call 2: Gemini formats results into response

Total: 2 calls
```

#### **Scenario 3: Multi-step (Logged in)**
```
User: "I want to return my laptop"
â”œâ”€ Call 1: Gemini decides to find user's orders
â”œâ”€ [Tool: find_orders_by_user_id]
â”œâ”€ Call 2: Gemini processes order list, decides to check eligibility
â”œâ”€ [Tool: check_return_eligibility]
â””â”€ Call 3: Gemini formats final answer

Total: 3 calls
```

#### **Scenario 4: Complex with RAG**
```
User: "My laptop is overheating, can I return it?"
â”œâ”€ Call 1: Gemini finds user orders
â”œâ”€ [Tool: find_orders_by_user_id]
â”œâ”€ Call 2: Gemini checks return eligibility
â”œâ”€ [Tool: check_return_eligibility]
â”œâ”€ Call 3: Gemini searches troubleshooting
â”œâ”€ [Tool: get_troubleshooting_steps]
â”œâ”€ Call 4: Gemini searches policy docs (RAG)
â”‚   â”œâ”€ [RAG retrieval - not an LLM call]
â”‚   â””â”€ Call 5: RAG's answer_with_rag() calls Gemini
â””â”€ Call 6: Gemini synthesizes everything

Total: 6 calls
```

### **Average Statistics:**

| Query Type | Avg Iterations | Avg LLM Calls | Avg Tools Used |
|------------|----------------|---------------|----------------|
| Chitchat | 1 | 1 | 0 |
| Product Search | 2 | 2 | 1 |
| Order Status | 2 | 2 | 1-2 |
| Return/Refund | 3 | 3 | 2-3 |
| Complex Multi-step | 4-5 | 5-6 | 3-5 |

### **Max Iterations Safety:**

```python
# In orchestrator.py
for iteration in range(10):  # MAX 10 to prevent infinite loops
    ...
```

**Why 10?**
- âœ… Handles 99% of queries (most need 2-4)
- âœ… Prevents runaway costs
- âœ… Timeout protection
- âœ… User experience (fast responses)

---

## **1ï¸âƒ£2ï¸âƒ£ Scenario-Based Questions for Viva**

### **Q1: What if ChromaDB goes down?**
**A:** Fallback mechanism:
1. Try RAG first
2. If fails, use LLM's parametric knowledge
3. If LLM uncertain, return: "I need to check our documentation. Please contact support."

### **Q2: How do you handle concurrent users?**
**A:** 
- SQLite supports concurrent reads
- Each session is isolated (session_id)
- No shared state between users
- Can scale to ~100 concurrent users
- For production: Migrate to PostgreSQL + Redis

### **Q3: What if user clears browser data?**
**A:**
- Loses session_id from localStorage
- New anonymous session created
- Previous history lost (unless logged in)
- If logged in: Can create new session with same user_id

### **Q4: How do you prevent prompt injection?**
**A:**
1. Guardrails filter malicious inputs
2. System prompts have clear boundaries
3. Tools are sandboxed (can't execute arbitrary code)
4. No `eval()` or dynamic code execution
5. User input never becomes code

### **Q5: What's your token cost per query?**
**A:**
```
Gemini 2.0 Flash Pricing:
- Input: $0.10 / 1M tokens
- Output: $0.30 / 1M tokens

Average query:
- Input: 8,000 tokens = $0.0008
- Output: 700 tokens = $0.00021
- Total: $0.001 per query

1000 queries = $1
```

### **Q6: How would you add a new tool?**
**A:**
```python
# Step 1: Define in tool_definitions.py
{
    "name": "cancel_order",
    "description": "Cancel an order",
    "parameters": {...}
}

# Step 2: Implement in tools/
def cancel_order_tool(order_id):
    ...

# Step 3: Add to orchestrator.py execute_tool_call()
tool_map = {
    ...
    "cancel_order": cancel_order_tool
}
```

### **Q7: How do you evaluate RAG quality?**
**A:**
- **Retrieval Eval:** Recall@3, Precision@3
- **Generation Eval:** ROUGE-L (overlap with ground truth)
- **Human Eval:** Accuracy, Helpfulness, Relevance
- See `retrieval_eval.py` and `generation_eval.py`

### **Q8: Why SQLite instead of PostgreSQL?**
**A:**
- âœ… Zero setup (file-based)
- âœ… Perfect for demo/capstone
- âœ… Handles 100s of users
- âœ… Easy to show evaluators
- âš ï¸ Production: Would use PostgreSQL

### **Q9: What if Gemini API rate limits you?**
**A:**
- Implement exponential backoff
- Queue requests
- Fallback to cached responses
- Show user: "High traffic, please wait..."

### **Q10: How would you add voice input?**
**A:**
```javascript
// Frontend
navigator.mediaDevices.getUserMedia({audio: true})
  .then(stream => {
    const recognition = new webkitSpeechRecognition();
    recognition.onresult = (e) => {
      const transcript = e.results[0][0].transcript;
      sendMessage(transcript);
    };
  });
```

---

## **ğŸ¯ Quick Reference Card for Viva**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SYSTEM OVERVIEW                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Architecture: RAG + Agentic + Tools + Guardrails       â”‚
â”‚ LLM: Gemini 2.0 Flash Experimental                     â”‚
â”‚ Vector DB: ChromaDB (Chroma)                           â”‚
â”‚ Embedding: all-MiniLM-L6-v2 (384 dims)                 â”‚
â”‚ Database: SQLite (assistant.db)                        â”‚
â”‚ Framework: FastAPI + Vanilla JS                        â”‚
â”‚ Agent Pattern: Gemini Function Calling (ReAct-style)   â”‚
â”‚                                                          â”‚
â”‚ KEY METRICS                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chunk Size: 1000 chars (overlap 200)                   â”‚
â”‚ Retrieval: Top-3 (Cosine similarity)                   â”‚
â”‚ History: Last 20 messages                              â”‚
â”‚ Max Iterations: 10 per query                           â”‚
â”‚ Avg LLM Calls: 2-3 per query                           â”‚
â”‚ Tokens/Query: ~8k input, ~700 output                   â”‚
â”‚ Response Time: 2-5 seconds                             â”‚
â”‚                                                          â”‚
â”‚ AGENTIC PROOF                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… LLM autonomously decides which tools to call         â”‚
â”‚ âœ… Can chain 3-5 tools in sequence                      â”‚
â”‚ âœ… Self-corrects based on tool results                  â”‚
â”‚ âœ… Adapts strategy mid-conversation                     â”‚
â”‚                                                          â”‚
â”‚ PRODUCTION READINESS                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Authentication & Authorization                        â”‚
â”‚ âœ… Session Management                                    â”‚
â”‚ âœ… Conversation Memory                                   â”‚
â”‚ âœ… Error Handling & Fallbacks                           â”‚
â”‚ âœ… Guardrails (Safety, Domain, Inappropriate)          â”‚
â”‚ âš ï¸  Would add: Rate limiting, Caching, Monitoring      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Good luck! You've built a production-quality system!** ğŸš€
